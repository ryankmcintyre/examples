== Automated Placement


This example illustrates the ways how the placement of scheduled pods can be influenced.
For this examples we assume that you have Minikube installed and running as described in link:../../INSTALL.adoc#minikube[here].

=== Node Selector

The simplest way to influence the scheduling process is to use a `nodeSelector`.

Apply our simple example https://github.com/k8spatterns/random-generator[random-generator] application with node selector:

[source, bash]
----
kubectl create -f node-selector.yml
----

You will notice that this Pod does not get scheduled, because the `nodeSelector` can't find any node with the label `disktype=ssd`:

[source, bash]
----
kubectl describe pod node-selector
....
Events:
  Type     Reason            Age              From               Message
  ----     ------            ----             ----               -------
  Warning  FailedScheduling  8s (x2 over 8s)  default-scheduler  0/1 nodes are available: 1 node(s) didnt match node selector.
----

Let's change this (the below adds a label to the node in index 0 of the nodes collection):

[source, bash]
----
kubectl label node $(kubectl get nodes -o jsonpath="{$.items[0].metadata.name}") disktype=ssd

kuebctl get pods
NAME                READY   STATUS    RESTARTS   AGE
random-generator    1/1     Running   0          65s
----

=== Node Affinity

Let's now use Node affinity rules for scheduling our Pod:

[source, bash]
----
kubectl create -f node-affinity.yml
----

Again, our Pod won't schedule as there is no node which fullfills the affinity rules.
We can change this with

[source, bash]
----
kubectl label node $(kubectl get nodes -o jsonpath="{$.items[0].metadata.name}") numberCores=4
----

Does the Pod starts up now ? What if you choose 2 instead of 4 for the number of cores ?

=== Pod Affinity

To test Pod affinity we need to install a Pod to which our Pod can be connected.
We are trying to create both Pods with

[source, bash]
----
kubectl create -f pod-affinity.yml

pod/pod-affinity created
pod/confidential-high created

kubctl get pods
NAME                READY   STATUS    RESTARTS   AGE
confidential-high   1/1     Running   0          22s
pod-affinity        0/1     Pending   0          22s
----

"confidential-high" is a placeholder pod which has a label which is matched by our "pod-affinity" Pod.
However our node doesn't have the proper topology key yet.
That can be changed with

[source, bash]
----
kubectl label --overwrite node $(kubectl get nodes -o jsonpath="{$.items[0].metadata.name}") security-zone=high

node/aks-nodepool1-{some ID}-0 labeled

kubectl get pods
NAME                READY   STATUS    RESTARTS   AGE
confidential-high   1/1     Running   0          9m39s
pod-affinity        1/1     Running   0          9m39s
----

(If the `pod-affinity` pod still does not start, check the placement of the `confidential-high` pod as it may be on a different node than where the security-zone label was applied.)

=== Taints and Tolerations

For testing taints and tolerations, we first have to taint our AKS node(s), so that by default no Pods are scheduled on it. If you have more than one node in your AKS cluster, run the command below for each node changing the index value on `items`:

[source, bash]
----
kubectl taint nodes $(kubectl get nodes -o jsonpath="{$.items[0].metadata.name}") node-role.kubernetes.io/master="":NoSchedule
----

You can check that this taint is working by reapplying the previous `pod-affinity.yml` example and see that the `confidential-high` pod is not scheduled.

[source, bash]
----
kubectl delete -f pod-affinity.yml
kubectl create -f pod-affinity.yml

kubectl get pods

NAME                READY   STATUS    RESTARTS   AGE
confidential-high   0/1     Pending   0          2s
pod-affinity        0/1     Pending   0          2s
----

But our Pod in `tolerations.yml` can be scheduled as it tolerates this new taint on Minikube:

[source, bash]
----
kubectl create -f tolerations.yml

kubectl get pods
NAME                READY   STATUS    RESTARTS   AGE
confidential-high   0/1     Pending   0          2m51s
pod-affinity        0/1     Pending   0          2m51s
tolerations         1/1     Running   0          4s
----

=== More Information

* https://kubernetes.io/docs/user-guide/node-selection/[Assigning Pods to Nodes]
* http://kubernetes.io/docs/admin/multiple-zones/[Running in Multiple Zones]
* https://blog.openshift.com/node-placement-scheduling-explained/[Node Placement and Scheduling Explained]
* https://kubernetes.io/docs/concepts/configuration/assign-pod-node/[Assigning Pods to Nodes]
* https://kubernetes.io/docs/admin/disruptions/[Pod Disruption Budget by Kubernetes]
* https://kubernetes.io/docs/admin/rescheduler/[Guaranteed Scheduling For Critical Add-On Pods]
* https://docs.okd.io/latest/admin_guide/scheduling/scheduler.html[The Kubernetes Scheduler]
* https://github.com/kubernetes/community/blob/master/contributors/devel/scheduler_algorithm.md[Scheduler Algorithm]
* https://kubernetes.io/docs/admin/multiple-schedulers/[Configuring Multiple Schedulers]
* https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node-allocatable.md[Node Allocatable Resources]
* https://github.com/kubernetes-incubator/descheduler[Descheduler for Kubernetes]
* https://www.youtube.com/watch?v=nWGkvrIPqJ4[Everything You Ever Wanted to Know About Resource Scheduling, But Were Afraid to Ask]
